{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "w_nb = 20 # number of words before linebreaks in output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ཧྲོད\n"
     ]
    }
   ],
   "source": [
    "# import the lexicon\n",
    "with open('tsikchen.txt', 'r', -1, 'utf-8-sig') as f:\n",
    "    lexicon = [line.strip() for line in f.readlines()]\n",
    "# add all the particles\n",
    "lexicon.extend(['གི', 'ཀྱི', 'གྱི', 'ཡི', 'གིས', 'ཀྱིས', 'གྱིས', 'ཡིས', 'སུ', 'ཏུ', 'དུ', 'རུ', 'སྟེ', 'ཏེ', 'དེ', 'ཀྱང', 'ཡང', 'འང', 'གམ', 'ངམ', 'དམ', 'ནམ', 'བམ', 'མམ', 'འམ', 'རམ', 'ལམ', 'སམ', 'ཏམ', 'གོ', 'ངོ', 'དོ', 'ནོ', 'མོ', 'འོ', 'རོ', 'ལོ', 'སོ', 'ཏོ', 'ཅིང', 'ཅེས', 'ཅེའོ', 'ཅེ་ན', 'ཅིག', 'ཞིང', 'ཞེས', 'ཞེའོ', 'ཞེ་ན', 'ཞིག', 'ཤིང', 'ཤེའོ', 'ཤེ་ན', 'ཤིག', 'ལ', 'ན', 'ནས', 'ལས', 'ནི', 'དང', 'གང', 'ཅི', 'ཇི', 'གིན', 'གྱིན', 'ཀྱིན', 'ཡིན', 'པ', 'བ', 'པོ', 'བོ'])\n",
    "# add all Monlam verbs\n",
    "with open('monlam1_verbs.txt', 'r', -1, 'utf-8-sig') as f:\n",
    "    monlam_verbs = [line.strip() for line in f.readlines()]\n",
    "for entry in monlam_verbs:\n",
    "    verb = entry.split(' | ')[0]\n",
    "    if verb not in lexicon:\n",
    "        lexicon.append(verb)\n",
    "\n",
    "# import the Monlam POS tags\n",
    "with open('./monlam1_pos.txt', 'r', -1, 'utf-8-sig') as f: # Monlam\n",
    "    monlam = [line.strip() for line in f.readlines()]\n",
    "monlam_pos = {}\n",
    "for line in monlam:\n",
    "    parts = line.split('—') # Monlam\n",
    "    monlam_pos[parts[0]] = parts[1] \n",
    "\n",
    "# import the Hill POS tags\n",
    "with open('hill_pos.txt', 'r', -1, 'utf-8-sig') as f: # Hill\n",
    "    hill = [line.strip() for line in f.readlines()]\n",
    "hill_pos = {}\n",
    "for line in hill:\n",
    "    parts = line.split('***') # Hill\n",
    "    hill_pos[parts[0]] = parts[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new_tsikchen = []\n",
    "no_pos = []\n",
    "for word in lexicon:\n",
    "    word = word[:-1]\n",
    "    if word in monlam_pos.keys() or word in hill_pos.keys():\n",
    "        new_tsikchen.append(word)\n",
    "    else:\n",
    "        no_pos.append(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with open('tsikchen.txt', 'w', -1, 'utf-8-sig') as f:\n",
    "    f.write('\\n'.join(new_tsikchen))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def isWord(syls):\n",
    "    maybe = '་'.join(syls)\n",
    "    final = False\n",
    "    if maybe in lexicon:\n",
    "        final = True\n",
    "    elif re.sub(merged_part, '', maybe) in lexicon:\n",
    "        final = True\n",
    "    return final\n",
    "\n",
    "def process(list1, list2, num):\n",
    "    word = '་'.join(list1[:num])\n",
    "    if word not in lexicon:\n",
    "        maybe = re.split(merged_part, word)\n",
    "        list2.append(maybe[0])\n",
    "        list2.append(maybe[1]+'་')\n",
    "        del list1[:num]\n",
    "    else:\n",
    "        list2.append(word+\"་\")\n",
    "        del list1[:num]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "881 words out of 1094 have a POS. \n",
      " 80.53016453382084 % of the words have tags.\n",
      "17 words out of 19 have a POS. \n",
      " 89.47368421052632 % of the words have tags.\n"
     ]
    }
   ],
   "source": [
    "for file in os.listdir('./IN/'):\n",
    "    if file.endswith(\".txt\"):\n",
    "    #todo - replace \\n by \\s try: with open('drugs') as temp_file: \\n drugs = [line.rstrip('\\n') for line in temp_file]\n",
    "        try:\n",
    "            with open('./IN/' + file, 'r', -1, 'utf-8-sig') as f:\n",
    "                current_file = f.read().replace('\\n', '').replace('\\r\\n', '')\n",
    "\n",
    "        except:\n",
    "            print(\"Save all IN files as UTF-8 and try again.\")\n",
    "            input()\n",
    "    else:\n",
    "        print(\"\\nSave all IN files as text files and try again.\")\n",
    "        input()\n",
    "    \n",
    "    ######################\n",
    "    # Segmentation process\n",
    "    merged_part = r'(ར|ས|འི|འམ|འང)$'\n",
    "    \n",
    "    syls = re.sub(r\"([།|༎|༏|༐|༑|༔|\\s]+)\", \"་\\g<1>་\", current_file)\n",
    "    syls = re.split(r\"་+\", syls)\n",
    "    \n",
    "    non_words = []\n",
    "    words = []\n",
    "    while len(syls) > 0:\n",
    "        if   isWord(syls[:4]): process(syls, words, 4)\n",
    "        elif isWord(syls[:3]): process(syls, words, 3)\n",
    "        elif isWord(syls[:2]): process(syls, words, 2)\n",
    "        elif isWord(syls[:1]): process(syls, words, 1)\n",
    "        else:\n",
    "            words.append('་'.join(syls[:1])+\"་*\")\n",
    "            non_words.append('་'.join(syls[:1])+\"་\")\n",
    "            del syls[:1]\n",
    "    #\n",
    "    ######################\n",
    "    pos_tagged = []\n",
    "    for word in words:\n",
    "        wor = word\n",
    "        if word.endswith('་'):\n",
    "            wor = word[:-1]\n",
    "        if wor in monlam_pos.keys() and wor in hill_pos.keys():\n",
    "            pos_tagged.append(word+'|'+hill_pos[wor]+'#'+monlam_pos[wor])\n",
    "        elif wor in monlam_pos.keys() and wor not in hill_pos.keys():\n",
    "            pos_tagged.append(word+'|'+monlam_pos[wor])\n",
    "        elif wor not in monlam_pos.keys() and wor in hill_pos.keys():\n",
    "            pos_tagged.append(word+'|'+hill_pos[wor])\n",
    "        else:\n",
    "            pos_tagged.append(word)\n",
    "    \n",
    "    ######################\n",
    "    # count percentage of POS tagged words\n",
    "    \n",
    "    pos = 0\n",
    "    no_pos = 0\n",
    "    for word in pos_tagged:\n",
    "        if '|' in word:\n",
    "            pos = pos+1\n",
    "        else:\n",
    "            no_pos = pos+1\n",
    "    print(pos, 'words out of', len(pos_tagged), 'have a POS.', '\\n', pos*100/len(pos_tagged), '% of the words have tags.')\n",
    "    \n",
    "    #\n",
    "    ######################\n",
    "    \n",
    "    ######################\n",
    "    # flag particles\n",
    "    \n",
    "    \n",
    "    #\n",
    "    ######################\n",
    "    \n",
    "    ######################\n",
    "    # flag verbs\n",
    "    \n",
    "    \n",
    "    #\n",
    "    ######################\n",
    "    \n",
    "\n",
    "    # add linebreaks after 400 words\n",
    "    for i in range(w_nb-1, len(pos_tagged), w_nb):\n",
    "        words[i] += '\\n'\n",
    "    \n",
    "    # write output\n",
    "    with open('./OUT/' + 'seg_' + file, 'w', -1, 'utf-8-sig') as f:\n",
    "        f.write(' '.join(words))\n",
    "    \n",
    "    with open('nonwords_' + file, 'w', -1, 'utf-8-sig') as f:\n",
    "        f.write('\\n'.join(non_words))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
