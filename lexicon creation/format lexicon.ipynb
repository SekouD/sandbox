{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hill Lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./2016-04-06-lexicon.txt', 'r', -1, 'utf-8-sig') as f:\n",
    "    content = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lexicon = {}\n",
    "for line in content:\n",
    "    parts = line.split('\\t')\n",
    "    entry = parts[0]\n",
    "    pos = [elt.strip(' -') for elt in parts[1:]]\n",
    "    if entry.endswith('་'):\n",
    "        entry = entry[:-1]\n",
    "    if entry in lexicon.keys():\n",
    "        for p in pos:\n",
    "            if p not in lexicon[entry]:\n",
    "                lexicon[entry].append(p)\n",
    "    else:\n",
    "        lexicon[entry] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "flat_lexicon = {}\n",
    "for entry in lexicon.keys():\n",
    "    flat_lexicon[entry] = '_'.join(lexicon[entry])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pos_list = []\n",
    "for entry in lexicon.keys():\n",
    "    pos_list.extend(lexicon[entry])\n",
    "pos_set = set(pos_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(sorted(pos_set))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('hill_lexicon.txt', 'w', -1, 'utf-8-sig') as f:\n",
    "    for entry in no_prop.keys():\n",
    "        f.write(entry+'***'+no_prop[entry]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cases = {}\n",
    "for entry in flat_lexicon.keys():\n",
    "    pos = flat_lexicon[entry]\n",
    "    if 'case' in pos and 'cv' not in pos:\n",
    "        print('nocv: ', entry, pos)\n",
    "    #elif 'case' not in pos and 'cv' in pos:\n",
    "     #   print('nocase: ', entry, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for entry in flat_lexicon.keys():\n",
    "    pos = flat_lexicon[entry]\n",
    "    if 'case' not in pos and 'cv' in pos:\n",
    "        print('nocase: ', entry, pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "no_prop = {}\n",
    "prop = []\n",
    "for entry in flat_lexicon.keys():\n",
    "    pos = flat_lexicon[entry]\n",
    "    if 'prop' in pos:\n",
    "        prop.append(entry)\n",
    "    else:\n",
    "        no_prop[entry] = pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(prop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(one_pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "len(flat_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "22279-15963"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "6316*100/22279"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Monlam Lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as Soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('monlamTb1.xml', 'r', -1, 'utf-8-sig') as f:\n",
    "    monlam = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "soup = Soup(monlam, 'lxml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dicts = soup.array.findAll('dict')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "monlam_dict = []\n",
    "for dic in dicts:\n",
    "    entry = dic.find_all('string')\n",
    "    lemma = entry[0].string\n",
    "    descr = entry[1].string.replace('\\n', '')\n",
    "    monlam_dict.append(lemma+' | '+descr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./monlam/monlam1_total.txt', 'w', -1, 'utf-8-sig') as f:\n",
    "    f.write('\\n'.join(monlam_dict))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## From txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('./monlam/monlam1_total.txt','r', -1, 'utf-8-sig') as f:\n",
    "    content0 = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### find all Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combined_entries = []\n",
    "for line in content0:\n",
    "    # separate the entry and the description\n",
    "    parts = line.split(' | ')\n",
    "    lemma = parts[0].strip()\n",
    "    #print(lemma, end = ' ')\n",
    "    # separate all sub-entries\n",
    "    sub_entries = re.split(r'༡(མིང|བྱ|རྒྱན|བསྣན|ཚབ|མཚུངས|སྒྲུབ|ཕྲད|འབོད|གྲངས|འཇལ|གྲོགས)་ཚིག', parts[1])\n",
    "    tags = ''\n",
    "    for i in range(1, len(sub_entries), 2):\n",
    "        tags += sub_entries[i]+'+'\n",
    "    if tags not in combined_entries:\n",
    "        combined_entries.append(tags)\n",
    "        if tags == '':\n",
    "            print(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print(combined_entries)\n",
    "for e in combined_entries:\n",
    "    plus = re.findall('\\+', e)\n",
    "    print(plus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### correct missing tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# list all the pos by looking at the \n",
    "pos = []\n",
    "for line in content0:\n",
    "    tags = re.findall(r' [༡༢][^ ༡༢༣༤༥༦༧༨༩༠.༽]+[ །]', line)\n",
    "    pos.extend(tags)\n",
    "pos = sorted(set(pos))\n",
    "\n",
    "for p in pos:\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_missing = []\n",
    "num_inserted = []\n",
    "# insert the missing number in all tags that lack it\n",
    "for num, line in enumerate(content0):\n",
    "    tag = re.findall(r'[། ][^༡༢](ཆོས་ལུགས།|ལོ་རྒྱུས།|ལྟ་གྲུབ།|རྩོམ་རིག|གསོ་རིག|ཆབ་སྲིད།|ཚན་རིག|བཟོ་རིག|རྩིས་རིག|ཁྲིམས་ལུགས།|དཔལ་འབྱོར།|དམག་དོན།|འཛིན་སྐྱོང་།|སྒྲ་རིག་པ།|ཤེས་ཡོན།|སློབ་གསོ།|ཚད་མ།|ཚད་མ་རིག་པ།|སྐྱེ་དངོས་རིག་པ།|ཟུར་ཆག|རྒྱ་གར།|རྒྱ་ནག|རྒྱ་ནག།|སོག་པོ།|མན་ཇུ།|དབྱིན་ཇི།|བལ་བོ།|མིང་གི་རྣམ་གྲངས།|མིང་གི་གྲངས།|མིང་གི་རྣམ་གྲངས་ལ།|ལྡོག་ཚིག|ལྡོག་ཚིིག|ལྡོག་ཚིག|ཉེ་ཚིག|ཟུར་མཆན།|ཡུལ་སྐད།|ཞེ་ས།|བརྙས་ཚིག|ལོག་སྐད།|དཔེ་ཆོས།|མངོན་བརྗོད།|བརྡ་རྙིང་།|བསྡུས་ཚིག|རྒྱུན་སྤྱོད།|བསྡུར་རིམ།|མ་འོངས་པ།|མ་འོངས།|ད་ལྟ་བ་།|ད་ལྟ་བ།|འདས་པ།|འདས།|སྐུ་ཚིག|སྐུར་ཚིག|སྐུལ་ཚིག|སྐུལ་ཚིག།|སྐུུལ་ཚིག) ', line)\n",
    "    if tag != []:\n",
    "        truc = re.sub(r'([^༡༢])(ཆོས་ལུགས།|ལོ་རྒྱུས།|ལྟ་གྲུབ།|རྩོམ་རིག|གསོ་རིག|ཆབ་སྲིད།|ཚན་རིག|བཟོ་རིག|རྩིས་རིག|ཁྲིམས་ལུགས།|དཔལ་འབྱོར།|དམག་དོན།|འཛིན་སྐྱོང་།|སྒྲ་རིག་པ།|ཤེས་ཡོན།|སློབ་གསོ།|ཚད་མ།|ཚད་མ་རིག་པ།|སྐྱེ་དངོས་རིག་པ།|ཟུར་ཆག|རྒྱ་གར།|རྒྱ་ནག|རྒྱ་ནག།|སོག་པོ།|མན་ཇུ།|དབྱིན་ཇི།|བལ་བོ།|མིང་གི་རྣམ་གྲངས།|མིང་གི་གྲངས།|མིང་གི་རྣམ་གྲངས་ལ།|ལྡོག་ཚིག|ལྡོག་ཚིིག|ལྡོག་ཚིག|ཉེ་ཚིག|ཟུར་མཆན།|ཡུལ་སྐད།|ཞེ་ས།|བརྙས་ཚིག|ལོག་སྐད།|དཔེ་ཆོས།|མངོན་བརྗོད།|བརྡ་རྙིང་།|བསྡུས་ཚིག|རྒྱུན་སྤྱོད།|བསྡུར་རིམ།|མ་འོངས་པ།|མ་འོངས།|ད་ལྟ་བ་།|ད་ལྟ་བ།|འདས་པ།|འདས།|སྐུ་ཚིག|སྐུར་ཚིག|སྐུལ་ཚིག|སྐུལ་ཚིག།|སྐུུལ་ཚིག) ', r'\\1༡\\2 ', line)\n",
    "        num_inserted.append(str(num)+truc)\n",
    "        num_missing.append(str(num)+line)\n",
    "with open('monlam_num_missing.txt', 'w', -1, 'utf-8-sig') as f:\n",
    "    for line in num_missing:\n",
    "        f.write(line+'\\n')\n",
    "with open('monlam_num_inserted.txt', 'w', -1, 'utf-8-sig') as f:\n",
    "    for line in num_inserted:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "to do : compare in Meld the two, put a '*' at the beginning of each line where the ༡ is not necessary, and delete the extra ones in lines where needed. Save the files in the monlam folder to not risk delete the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./monlam/monlam_num_inserted.txt', 'r', -1, 'utf-8-sig') as f:\n",
    "    inserted = [line.strip() for line in f.readlines()]\n",
    "monlam_corrected = content0\n",
    "\n",
    "# replace the bad entries by the ones from monlam_num_inserted.txt\n",
    "for line in inserted:\n",
    "    if not line.startswith('*'):\n",
    "        entry = [i for i in re.split('^([0-9]+)', line) if i != '']\n",
    "        monlam_corrected[int(entry[0])] = entry[1]\n",
    "\n",
    "with open('monlam1_total_corrected.txt', 'w', -1, 'utf-8-sig') as f:\n",
    "    for line in monlam_corrected:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('monlam1_total_corrected.txt', 'r', -1, 'utf-8-sig') as f:\n",
    "    content1 = [line.strip() for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def isRef(string):\n",
    "    test = False\n",
    "    if re.findall(r'[^༡༢](མ་འོངས་པ།|ད་ལྟ་བ།|འདས་པ།|སྐུལ་ཚིག)', string):\n",
    "        test = True\n",
    "    return test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add2dict(key, value, dic):\n",
    "    if key not in dic.keys():\n",
    "        dic[key] = value\n",
    "    else:\n",
    "        dic[key] = list(set(dic[key] + value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "verbs = []\n",
    "rest = []\n",
    "for line in content1:\n",
    "    if '༡བྱ་ཚིག' in line and '༡རྒྱ་གར།' not in line:\n",
    "        verbs.append(line)\n",
    "    else:\n",
    "        rest.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('monlam1_nonverbs.txt', 'w', -1, 'utf-8-sig') as f:\n",
    "    for line in rest:\n",
    "        f.write(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "thadepa = 'ཐ་དད་པ'\n",
    "thamidepa = 'ཐ་མི་དད་པ'\n",
    "zugmigyur = 'གཟུགས་མི་འགྱུར་བ'\n",
    "empty = '###'\n",
    "unknown = 'བྱ་ཚིག'\n",
    "attr_sep = '_'\n",
    "pos_sep = '+'\n",
    "\n",
    "verbs_dict = {}\n",
    "for verb in verbs:\n",
    "    # separate the entry and the description\n",
    "    parts = verb.split(' | ')\n",
    "    lemma = parts[0].strip().replace('་', '')\n",
    "    \n",
    "    # separate all sub-entries\n",
    "    sub_entries = re.split(r'༡(མིང|བྱ|རྒྱན|བསྣན|ཚབ|མཚུངས|སྒྲུབ|ཕྲད|འབོད|གྲངས|འཇལ|གྲོགས)་ཚིག', parts[1])\n",
    "    # only keep the verb sub-entry\n",
    "    sub = ''\n",
    "    for i in range(len(sub_entries)):\n",
    "        if sub_entries[i] == 'བྱ':\n",
    "            sub = sub_entries[i+1]\n",
    "    \n",
    "    # separate meanings\n",
    "    meanings = re.split(r' [0-9]+\\. ', sub)\n",
    "    \n",
    "    # keep only verbs that are not references\n",
    "    new_meanings = [m.strip() for m in meanings if not isRef(m) and m != '']\n",
    "    \n",
    "    if new_meanings != []:\n",
    "        for meaning in new_meanings:\n",
    "            #print(lemma, meaning)\n",
    "            # ཐ་དད་པ་ or ཐ་མི་དད་པ་\n",
    "            tha = empty\n",
    "            \n",
    "            if  'བྱ་བྱེད་ཐ་མི་དད་པ། ' in meaning: tha = thamidepa\n",
    "            elif 'བྱ་བྱེད་ཐ་དད་པ། ' in meaning: tha = thadepa\n",
    "            \n",
    "            # tenses\n",
    "            if ' གཟུགས་མི་འགྱུར་བ། ' in meaning:\n",
    "                add2dict(lemma, [zugmigyur+attr_sep+tha], verbs_dict)\n",
    "                #print({lemma : zugmigyur+attr_sep+tha})\n",
    "            else:\n",
    "                if 'མ་འོངས་པ།' in meaning or 'ད་ལྟ་བ།' in meaning or 'འདས་པ།' in meaning or 'སྐུལ་ཚིག' in meaning:\n",
    "                    tenses = re.findall(r'.(མ་འོངས་པ།|མ་འོངས།) ([^\\s]+) .(ད་ལྟ་བ་།|ད་ལྟ་བ།|ལྟ་བ།) ([^\\s]+) .(འདས་པ།|འདས།) ([^\\s]+) .?(སྐུ་ཚིག|སྐུར་ཚིག|སྐུལ་ཚིག|སྐུལ་ཚིག།|སྐུུལ་ཚིག)? ?([^\\s]+)?\\s+དཔེར་ན།', meaning)\n",
    "\n",
    "                    ############################\n",
    "                    # hack because of a strange behaviour of python : the structure of tenses is a tuple embedded in a list\n",
    "                    for t in tenses: \n",
    "                        temp = []\n",
    "                        for elt in t:\n",
    "                            if elt != '':\n",
    "                                temp.append(elt)\n",
    "                    tenses = temp\n",
    "                    #\n",
    "                    ############################\n",
    "\n",
    "                    #print(tenses)\n",
    "                    for i in range(0, len(tenses)-1, 2):\n",
    "                        inflected = tenses[i+1].replace('།', '').replace('་', '')\n",
    "                        tense = tenses[i]\n",
    "                        add2dict(inflected, [lemma+attr_sep+tense+attr_sep+tha], verbs_dict)\n",
    "                        #print({inflected : lemma+attr_sep+tense+attr_sep+tha})\n",
    "                else:\n",
    "                    add2dict(lemma, [unknown+attr_sep+tha], verbs_dict)\n",
    "                    #print({lemma : unknown+attr_sep+tha})\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# flatten the pos list into a string\n",
    "for v in verbs_dict.keys():\n",
    "    flattened = '+'.join(verbs_dict[v])\n",
    "    verbs_dict[v] = flattened"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'གསུང_མ་འོངས་པ།_ཐ་དད་པ+གསུང_ད་ལྟ་བ།_ཐ་དད་པ'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verbs_dict['གསུང']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "with open('monlam1_verbs.txt', 'w', -1, 'utf-8-sig') as f:\n",
    "    for verb in sorted(verbs_dict.keys()):\n",
    "        f.write(verb+' | '+verbs_dict[verb]+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
